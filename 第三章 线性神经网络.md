## 3.1 线性回归

### 3.1.1 线性回归基本元素
单个预测值的线性回归可表示为：
$$
\hat y = \textbf{w}^\top \textbf{x} + b
$$
若对特征集合，预测值是向量：
$$
\hat{\mathbf{y}} = \mathbf{X} \mathbf{w} + b
$$
其中 $\textbf{w}_{d \times 1}$ 是权重向量

定义损失函数：
$$
L(\mathbf{w}, b) = \frac{1}{n} \sum_{i=1}^{n} l^{(i)}(\mathbf{w}, b) = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{2} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right)^2
$$
我们希望找到一组合适的 $(\textbf{w}, b)$，使得损失函数 $\| \textbf{y} -\textbf{Xw} \|^2$ 最小，这可以找到**解析解(analytical solution)**：
$$
\mathbf{w}^* = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}.
$$
解析解是数学上的精确解，大部分时候我们的模型找不到这样的解，因此需要用**梯度下降 (gradient descent)** 来解决

随机梯度下降的算法如下：
1. 初始化模型参数的值，如随机初始化
2. 从发数据集中抽取小批量样本 $\mathcal{B}$ (遍历数据集运算过大)，在负梯度方向更新参数
3. 迭代，直到收敛
数学表达如下：
$$
\begin{align*}
&\mathbf{w} \leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right), \\
&b \leftarrow b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b) = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right)
\end{align*}
$$
这里 $|\mathcal{B}|$ 表示小批量中的样本数，即**批量大小 (batch size)**；$\eta$ 表示**学习率 (learning rate)**。这两个值是手动指定的。在未见过的数据集上实现较低的损失就是**泛化 (generalization)**
> 梯度下降即对参数向量 $\textbf{w}$ 减去学习率乘以梯度，梯度即损失函数的偏导数向量

### 3.1.2 矢量化加速
在 `torch` 框架下的矢量运算速度要比 `Python` 自带的 `for` 循环快得多。我们首先定义一个计算时间记录器：
```python
class Timer:  #@save
    """记录多次运行时间"""
    def __init__(self):
        self.times = []
        self.start()

    def start(self):
        """启动计时器"""
        self.tik = time.time()

    def stop(self):
        """停止计时器并将时间记录在列表中"""
        self.times.append(time.time() - self.tik)
        return self.times[-1]

    def avg(self):
        """返回平均时间"""
        return sum(self.times) / len(self.times)

    def sum(self):
        """返回时间总和"""
        return sum(self.times)

    def cumsum(self):
        """返回累计时间"""
        return np.array(self.times).cumsum().tolist()
```
下面的代码比较了 `for` 循环计算矢量加法和 `torch` 框架矢量化运算的花费时间：
```python
#定义两个向量
n = 10000
a = torch.ones([n])
b = torch.ones([n])

#for循环耗时
c = torch.zeros(n)
timer = Timer()
for i in range(n):
    c[i] = a[i] + b[i]
f'{timer.stop():.5f} sec'

#矢量运算耗时
timer.start()
d = a + b
f'{timer.stop():.5f} sec'
```
### 3.1.3 正态分布与平方损失
用均方误差衡量线性回归的原因是：假设观测值包含噪声，噪声服从正态分布：
$$
y = \textbf{w}^\top \textbf{x} + b + \epsilon
$$
其中 $\epsilon \sim \mathcal{N}(0, \sigma^2)$。因此，我们可以写出给定 $\textbf{x}$ 观测到 $y$ 的**似然 (likelihood)**：
$$
P(y\mid\mathbf{x})=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(y-\mathbf{w}^\top\mathbf{x}-b)^2\right)
$$
根据极大似然估计法，我们需要估计参数的最优值是使数据集似然最大的值：
$$
P(\mathbf{y}\mid\mathbf{X})=\prod_{i=1}^np(y^{(i)}|\mathbf{x}^{(i)})
$$
这个估计可以改为最小化负对数似然以符合我们的运算要求：
$$
-\log P(\mathbf{y}\mid\mathbf{X})=\sum_{i=1}^n\frac{1}{2}\log(2\pi\sigma^2)+\frac{1}{2\sigma^2}\left(y^{(i)}-\mathbf{w}^\top\mathbf{x}^{(i)}-b\right)^2
$$

## 3.2 从零开始实现线性回归
这里生成一个 $\textbf{X} \in \mathbb{R}^{1000 \times 2}$ 的样本，参数为 $\textbf{w} = [2, -3.4]^\top, \space b=4.2$：
$$
\mathbf{y}=\mathbf{X}\mathbf{w}+b+\epsilon
$$
这里我们假设线性回归假设成立，并设置 $\sigma = 0.01$

生成数据集：
```python
def synthetic_data(w, b, num_examples):  #参数为预设线性回归参数和样本量
    """生成y=Xw+b+噪声"""
    X = torch.normal(0, 1, (num_examples, len(w))) #生成正态样本
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))

#创建真实数据
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000) #获取特征矩阵X和观测向量y
```
其中 `torch.matmul(X, w)` 表示运算矩阵乘法，`b` 通过广播机制加入到样本中

实现小批量样本获取：
```python
def data_iter(batch_size, features, labels):
    num_examples = len(features) #获取n×m的矩阵的样本数n
    indices = list(range(num_examples)) #索引列表, 表示样本的编号
    random.shuffle(indices) #随机打乱索引列表的顺序
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)]) #获取随机索引开始的batch, 若batch size不足以获取, 则获取所有样本
        yield features[batch_indices], labels[batch_indices] #返回元组

#样本batch采样示例        
batch_size = 10
for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break
```
这里的 `yield` 关键字返回一个生成器，每次迭代都可获得一个小批量特征及对应标签组成的元组。参见 [[2025年3月13日]]

为了使用随机梯度下降训练模型，我们首先需要：
1. 设定参数初始值
2. 定义模型
3. 定义损失函数
4. 定义优化方法
下面的代码实现了上述四个操作：
```python
#模型参数初始化
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

#定义模型
def linreg(X, w, b): 
    """线性回归模型"""
    return torch.matmul(X, w) + b
    
#定义损失函数
def squared_loss(y_hat, y): 
    """均方损失"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
    
#定义优化算法
def sgd(params, lr, batch_size): #params是需要优化的参数列表, lr表示学习率
    """小批量随机梯度下降"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size #归一化梯度, 因为每次用的是batch size的样本计算
            param.grad.zero_() #梯度清零
```
定义优化算法中的 `with` 语句是 `Python` 中的上下文管理器，此处表示不跟踪梯度计算，不需要计算梯度的梯度。参见 [[2025年3月13日]]

在训练中，每个**迭代周期 (epoch)** 使用 `data_iter` 来遍历数据集，并且将训练集中的所有样本都使用一次 (这里预先设置样本数量能被整除)。这里的迭代周期和学习率都是超参数，需要反复实验调整：
```python
#设定超参数
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

#迭代训练
for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)  # X和y的小批量损失
        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，
        # 并以此计算关于[w,b]的梯度
        l.sum().backward()
        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```
训练结束后，我们可以查看训练的估计误差：
```python
print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差: {true_b - b}')
```
这里因为我们已知真正参数，所以可以直接计算

## 3.3 线性回归简洁实现

现代深度学习框架可以更方便地实现神经网络的功能，包括数据迭代器、损失函数、优化器和神经网络层等

### 3.3.1-3.3.2 数据集
生成数据集：
```python
import numpy as np
import torch
from torch.utils import data #data.DataLoader是一个数据加载器类
from d2l import torch as d2l

#真实参数
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)
```

`torch` 框架拥有直接读取数据的 API：
```python
def load_array(data_arrays, batch_size, is_train=True): #data_arrays是多张量元组
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays) #将多个张量合并为一个数据集, 使用了解包操作
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)
```
此处的 `is_train` 参数表示指定每个迭代周期内打乱数据。现在，我们直接构造了一个可用于迭代器的类 `DataLoader`。使用 `iter()` 构造迭代器，并用 `next()` 逐项获取：
```python
print(next(iter(data_iter))) #这将输出第一个batch
```

### 3.3.3-3.3.4 模型定义和初始化
`torch` 中的**全连接层 (fully-connected layer)** 在 `nn.Linear` 类中定义：
```python
#nn是神经网络的缩写
from torch import nn

net = nn.Sequential(nn.Linear(2, 1)) #定义全连接层
```
这里的参数 `nn.Linear(2, 1)` 分别表示两个特征(特征的形状)和一个输出 (标量)

现在，我们创建了一个单层神经网络 `net`，它的一层是线性全连接层。初始化模型参数可以直接访问 `net` 的层来实现：
```python
#访问net的第一层
net[0].weight.data.normal_(0, 0.01) #设置系数权重数据, 按照正态分布随机采样
net[0].bias.data.fill_(0) #设置偏置项数据
```
这里的 `normal_` 和 `fill_` 是重写参数的替换方法

### 3.3.5-3.3.6 损失函数和优化算法
直接使用 `MSELoss` 类创建损失函数，它返回所有样本损失的平均值：
```python
loss = nn.MSELoss()
```

我们的优化算法使用随机梯度下降 (SGD)，这里直接创建训练器：
```python
trainer = torch.optim.SGD(net.parameters(), lr=0.03)
```
这个 `trainer` 对象需要指定优化的参数，我们已经通过 `net.parameters()` 访问得到；此外还要指定学习率

### 3.3.7 训练模型
这里通过 `for` 循环来迭代训练模型：
```python
# 训练模型
num_epochs = 3  # 定义训练的总轮数（epoch）
for epoch in range(num_epochs):  # 遍历每个 epoch
    for X, y in data_iter:  # 从 DataLoader 中逐批次获取数据
        l = loss(net(X), y)  # 计算当前网络预测值 net(X) 与真实标签 y 的损失
        trainer.zero_grad()  # 清空之前的梯度信息，避免梯度累积
        l.backward()         # 反向传播，计算梯度
        trainer.step()       # 根据梯度更新网络参数

    # 在每个epoch结束时，计算整个数据集上的损失
    l = loss(net(features), labels)  # 使用整个数据集计算损失
    print(f'epoch {epoch + 1}, loss {l:f}')  # 打印当前 epoch 的损失值
```
这里 `l.backward()` 更新梯度后梯度将会被 `trainer` 接受，此时 `trainer` 中包含当前梯度信息，接着使用 `trainer.step()` 根据梯度更新参数

现在，我们查看参数估计误差：
```python
#查看参数估计误差
w = net[0].weight.data
print('w的估计误差：', true_w - w.reshape(true_w.shape))
b = net[0].bias.data
print('b的估计误差：', true_b - b)
```

## 3.4 softmax 回归

### 3.4.1-3.4.3 softmax 回归及其网络
**softmax 回归**可用于分类问题。我们将可能的类别用**独热编码 (one-hot encoding)** 表示，它将分类和向量对应起来，向量的维度和类别一样多。例如分类“鸡”、“猫”、“狗”：
$$
y \in \{
(1, 0, 0), (0, 1, 0), (0, 0, 1)
\}
$$

分类问题将拥有多个输出，以输出每类分类的概率；这里的输出函数在数学上叫**仿射函数 (affine function)**。对于 $k$ 个特征和 $d$ 个分类，softmax 回归需要的权重数量为 $k \times d$，用向量表示为：
$$
\textbf{o} = \textbf{Wx} + \textbf{b}
$$

![[Pasted image 20250315101837.png]]

softmax 回归在图示中表现为一个全连接层，其参数开销 (用大 $O$ 表示法) 为：
$$
O(dq)
$$
其中 $d, q$ 分别表示输入神经元个数和输出神经元个数。在实践中，这个开销非常大，需要用超参数 $n$ 将其优化为：
$$
O(\frac{dq}{n})
$$
其中 $n$ 的选取有多种方法

### 3.4.4-3.4.5 softmax 运算与小批量样本矢量化
在上面的网络中，我们得到的输出结果可能出现以下问题：
1. 负的输出值
2. 输出值总和不为 1
这些错误违反概率论公理，需要**校准 (calibration)** 为正常的概率，这里使用 softmax 函数改变输出，并保持可导性质：
$$
\hat{\textbf{y}} = \text{softmax} (\textbf{o})
$$
其中：
$$
\hat y_i = \frac
{\exp(o_j)}
{\sum_k \exp(o_k)}
$$
即可满足 $0 \leq \hat y_j \leq 1$，所以可把 $\hat{\textbf{y}}$ 视为真正的概率分布

softmax 仍然是**线性模型 (linear model)**

GPU 可以进行批量地矢量化运算。设小批量样本 $\textbf{X}_{n \times d}$，权重 $\textbf{W}_{d \times q}$，偏置项 $\textbf{b}_{1 \times q}$，则矢量计算表达式：
$$
\begin{align*}
&\mathbf{O} = \mathbf{X}\mathbf{W} + \mathbf{b},
\\
&\hat{\mathbf{Y}} = \text{softmax}(\mathbf{O})
\end{align*}
$$
这里的输出 $\textbf{O}$ 和 $\hat{\textbf{Y}}$ 都是 $n \times d$ 的矩阵

### 3.4.6 损失函数
设数据集 $\{\textbf{X, Y}\}$，其中第 $i$ 对样本特征向量为 $\textbf{x}^{(i)}$，独热编码后的标签 $\textbf{y}^{(i)}$，则获得估计值的概率：
$$
P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^{n} P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
$$
据极大似然法，我们要最大化上面的概率，则相当于最小化负对数的下面的概率：
$$
-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^{n} -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}) = \sum_{i=1}^{n} l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)})
$$
这里的 $l(\textbf{y}^{(i)},\hat{\textbf{y}}^{(i)})$ 表示损失函数，且：
$$
l(\mathbf{y}, \hat{\mathbf{y}}) = -\sum_{j=1}^{q} y_j \log \hat{y}_j
$$
这个损失函数就是**交叉熵损失 (corss-entropy loss)**。在独热编码的条件下，$\textbf{y}$ 只有第 $k$ 个真实值为 1，其他值都是 0，因此：
$$
l(y,\hat{y})=-\log\hat{y}_k.
$$
因此我们只需要最小化交叉熵

将 softmax 的计算方式代入交叉熵损失中：
$$
\begin{aligned}
l(\mathbf{y},\hat{\mathbf{y}}) & =-\sum_{j=1}^qy_j\log\frac{\exp(o_j)}{\sum_{k=1}^q\exp(o_k)} \\
 & =\sum_{j=1}^qy_j\log\sum_{k=1}^q\exp(o_k)-\sum_{j=1}^qy_jo_j \\
 & =\log\sum_{k=1}^q\exp(o_k)-\sum_{j=1}^qy_jo_j.
\end{aligned}
$$
对未规范的预测 $o_j$ 求导，我们有：
$$
\partial_{o_j}l(\mathbf{y},\hat{\mathbf{y}})=\frac{\exp(o_j)}{\sum_{k=1}^q\exp(o_k)}-y_j=\mathrm{softmax}(\mathbf{o})_j-y_j.
$$
也即是模型分配的概率与实际发生情况之间的差异


### 3.4.7 信息论基础
**信息论 (information theory)** 涉及编码、解码、发送以及尽可能简洁地处理信息或数据

量化数据中的信息内容使用分布 $P$ 的**熵 (entropy)**，可通过方程：
$$
H[P]=\sum_j-P(j)\log P(j)
$$
得到。信息论基本定理之一指出，为了从分布 $p$ 中随机抽取的数据进行编码，我们至少需要 $H[P]$ 对其进行编码，这里的 $H[P]$ 是**纳特 (nat)**，相当于比特，但是以 $e$ 为底而非 $2$；因此 $1\space \text{nat}\approx 1.44 bit$

如果我们不能完全预测每一个事件, 那么我们有时可能会感到“惊异”。香农决定用信息量 $-\log P(j)$ 来量化这种惊异程度。在观察一个事件 $j$ 时,并赋予它 (主观) 概率 $P (j)$。当我们赋予一个事件较低的概率时, 我们的惊异会更大, 该事件的信息量也就更大。上式中定义的熵, 是当分配的概率真正匹配数据生成过程时的信息量的期望

记交叉熵 $H (P, Q)$ 表示“主观概率为 Q 的观察者在看到根据概率 P 生成的数据时的预期惊异”，则当 $P = Q$ 时，交叉熵最小为 $H (P, P) = H (P)$

### 3.4.8 模型预测和评估
下面的实验使用**精度 (accuracy)** 来评估模型性能，精度等于正确预测数与预测总数之比

## 3.5 图像分类数据集
本节使用基准数据集 `Fashion-MNIST`，这是早期的 `MNIST` 数据集的更新版
```python
import torch
import torchvision
from torch.utils import data
from torchvision import transforms #torch的扩展库, 用于视觉处理; transforms 用于图像预操作
from d2l import torch as d2l

d2l.use_svg_display() #设置一种绘图格式
```

### 3.5.1 数据集读取
用框架中的内置函数读取数据集：
```python
# 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，
# 并除以255使得所有像素的数值均在0～1之间
trans = transforms.ToTensor() #转换器, 将Python图形库中的图像转换为32位浮点数格式的张量
mnist_train = torchvision.datasets.FashionMNIST(
    root="../data", train=True, transform=trans, download=True)
mnist_test = torchvision.datasets.FashionMNIST(
    root="../data", train=False, transform=trans, download=True)
```
这里的 `download=True` 表明若指定路径没有数据集，则进行下载

`Fashion-MNIST` 数据集由 10 个类别的图像组成，每个类别的训练集为 6000 张图片，测试集为 1000 张图片：
```python
len(mnist_train), len(mnist_test)
```

检查图像形状：
```python
print(mnist_train[0][0].shape)
#输出torch.Size([1, 28, 28])
```
这里图像的大小为 $28\times28$，是单通道灰度图

这个函数用于数字标签索引和文本名称之间的转换：
```python
def get_fashion_mnist_labels(labels): 
    """返回Fashion-MNIST数据集的文本标签"""
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
    'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]
```
然后我们创建一个函数来可视化样本：
```python
def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
    """绘制图像列表"""
    figsize = (num_cols * scale, num_rows * scale) #按比例缩放图像大小
    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize) #创建num_rows × num_cols的子图
    axes = axes.flatten() #将子图展平, 方便迭代
    for i, (ax, img) in enumerate(zip(axes, imgs)): #对每个图像和子图
        if torch.is_tensor(img): #图片张量
            ax.imshow(img.numpy())
        else: #PIL图片
            ax.imshow(img)
        #删除所有的坐标
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    return axes
```
这里的函数参数 `imgs` 表示输入图像的列表，可以是 `torch.Tensor()`，也可以是 `PIL` 图像对象；`ax.imshow()` 显示热度图

使用函数来查看这些图像：
```python
#查看图像实例
X, y = next(iter(data.DataLoader(mnist_train, batch_size=18))) #创建迭代器
show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y))
```
这里的 `get_fashion_mnist_labels` 直接获取对应的标签

### 3.5.2 小批量读取
我们用内置的数据迭代器，每次小批量地读取数据：
```python
#小批量读取数据
batch_size = 256

def get_dataloader_workers():
    """使用4个进程来读取数据"""
    return 4

train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,
                             num_workers=get_dataloader_workers())
```
这里可以控制 `DataLoader` 读取时占用的进程资源，使用了 `num_workers` 参数

查看耗时：
```python
#查看读取耗时
timer = d2l.Timer()
for X, y in train_iter:
    continue
f'{timer.stop():.2f} sec'
```

### 3.5.3 整合所有组件
这里定义一个函数，用于获取和读取 `Fashion-MNIST` 数据集。它的返回值是训练集和验证集的数据迭代器，可选参数为调整形状大小：
```python
def load_data_fashion_mnist(batch_size, resize=None):  
    """下载Fashion-MNIST数据集，然后将其加载到内存中"""
    trans = [transforms.ToTensor()]
    if resize: #若提供了resize参数
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans) #将预处理操作整理为一个管道, 并一次执行管道中的操作
    mnist_train = torchvision.datasets.FashionMNIST(
        root="../data", train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root="../data", train=False, transform=trans, download=True)
        #返回迭代器
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                            num_workers=get_dataloader_workers()),
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers()))
```
验证函数功能，使用了 `resize` 参数：
```python
train_iter, test_iter = load_data_fashion_mnist(32, resize=64)
for X, y in train_iter:
    print(X.shape, X.dtype, y.shape, y.dtype)
    break
```

## 3.6 从零开始实现 softmax 回归
下面从底层实现 softmax 回归，这里使用到了刚才定义的获取数据集的函数：
```python
import torch
from IPython import display
from d2l import torch as d2l

#获取数据集
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

### 3.6.1 初始化参数
图像的 $2\times2$ 矩阵蕴含了图像的空间特征。为了处理方便，本节展平了图像，只把像素点看做不同特征：
```python
#定义输入神经元和输出神经元大小
num_inputs = 784 #特征维度28×28, 此处展平
num_outputs = 10

#设定初始参数
W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True) #权重矩阵784×10
b = torch.zeros(num_outputs, requires_grad=True) #偏置向量
```

### 3.6.2 定义 softmax 操作
回想 `sum` 运算符可以沿着张量的特定维度工作，也同时可以保留轴数：
```python
#关于sum的运算示例
X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
X.sum(0, keepdim=True), X.sum(1, keepdim=True)
```
这里的输出如下：
```python
#(tensor([[5., 7., 9.]]),
#tensor([[ 6.],
#         [15.]]))
```
可以发现，求和和矩阵的维度保留下来了

softmax 的三个步骤：
1. 对每项求幂，使用 `exp`
2. 对每行求和，得到每个样本的规范化常数
3. 将每一行除以其规范化常数，确保结果和为 1
即：
$$
\text{softmax} (\textbf{X})_{ij} = 
\frac
{\exp (\textbf{X}_{ij})}
{\sum_k \exp(\textbf{X}_{ik})}
$$
代码如下：
```python
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition  # 这里应用了广播机制
```
> 这里的 $\textbf{X}$ 表示输出矩阵，在之前的数学表达式中也即 $\textbf{Y}$，每一行表示一个样本的预测概率

使用函数效果如下：
```python
#softmax函数实例
X = torch.normal(0, 1, (2, 5))
X_prob = softmax(X)
print(X_prob, X_prob.sum(1)) #第二个输出为[1, 1]
```
> 注意上面函数的实现代码没有考虑极大值的可能的溢出，即精度不足

### 3.6.3-3.6.4 模型和损失函数定义
模型定义：
```python
def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
```
这里的 `X.reshape((-1, W.shape[0]))`：
* `-1` 表示自动考虑第一个轴的数量，以适应第二个轴的数量
* `W.shape[0]` 表示权重矩阵的行数，即特征数量
这是为了便于进行矩阵乘法 `torch.matmul()`

我们用交叉熵来定义损失函数，这里可用高级索引操作快速获取真实分类和估计分类。示例代码：
```python
#真实分类和估计分类示例
y = torch.tensor([0, 2])
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
print(y_hat[[0, 1], y])
#输出tensor([0.1000, 0.5000])
```
高级索引快速地找到了对应的真实类别估计概率，因此交叉熵损失函数可以如下定义：
```python
def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y])
    
#打印交叉熵
print(cross_entropy(y_hat, y))
```

### 3.6.5 分类精度
给定预测分布 `y_hat`，我们必须做出**硬预测 (hard prediction)**，通常选择预测概率最高那一类：
```python
def accuracy(y_hat, y):  
    """计算预测正确的数量"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1: #对于二维张量
        y_hat = y_hat.argmax(axis=1) #找到列中的最大估计
    cmp = y_hat.type(y.dtype) == y #cmp是一个张量, 值为True/False
    return float(cmp.type(y.dtype).sum()) #返回正确分类的数量
    
print(accuracy(y_hat, y) / len(y)) #输出精度
```
注意 `==` 运算符对数据类型敏感，因此 `y_hat` 的数据类型必须与 `y` 保持一致

对于任意数据迭代器 `data_iter` 可访问的数据集，我们现在可以评估 `net` 的精度了：
```python
def evaluate_accuracy(net, data_iter):
    """计算在指定数据集上模型的精度"""
    if isinstance(net, torch.nn.Module):
        net.eval()  #将模型设置为评估模式
    metric = Accumulator(2)  # 正确预测数、预测总数
    with torch.no_grad(): #禁用梯度计算, 节省内存
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
```
`net.eval()` 关闭了模型的随机性，例如 `dropout` 等

这里的 `Accumulator` 类用于对多个变量累加：
```python
class Accumulator: 
    """在n个变量上累加"""
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self): #复位
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx): #内建函数, 支持索引访问累加值(相当于重载运算符)
        return self.data[idx]
```
这个类实现了对一个长度为 `n` 的列表的累加。示例用法：
```python
# 创建一个 Accumulator 实例，用于累加 3 个变量
accumulator = Accumulator(3)

# 累加一些值
accumulator.add(1, 2, 3)
print(accumulator.data)  # 输出: [1.0, 2.0, 3.0]

accumulator.add(4, 5, 6)
print(accumulator.data)  # 输出: [5.0, 7.0, 9.0]

# 通过索引访问累加值
print(accumulator[0])  # 输出: 5.0
print(accumulator[1])  # 输出: 7.0
print(accumulator[2])  # 输出: 9.0

# 重置累加器
accumulator.reset()
print(accumulator.data)  # 输出: [0.0, 0.0, 0.0]
```

### 3.6.6 训练
定义函数，实现一个迭代周期的训练：
```python
def train_epoch_ch3(net, train_iter, loss, updater):
    """训练模型一个迭代周期（定义见第3章）"""
    if isinstance(net, torch.nn.Module): # 将模型设置为训练模式, 检查类别
        net.train() # 设置训练模式
    # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X) # 获取估计值
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer): # 检查是否为torch内置的训练器
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad() #清零梯度
            l.mean().backward()
            updater.step()
        else: # 否则使用自定义训练器
            #使用定制的优化器和损失函数
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    #返回训练损失和训练精度
    return metric[0] / metric[2], metric[1] / metric[2]
```
其中 `.numel()` 返回一个张量的元素总数；`return` 返回的是平均训练损失和训练精度，计算公式为总损失或总精度除以使用过的样本数量

这里实现了一个绘制数据的实用程序类 `Animator`：
```python
class Animator:  #@save
    """在动画中绘制数据"""
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)): # fmts设置了绘图样式
        # 增量地绘制多条线
        if legend is None:
            legend = []
        d2l.use_svg_display() # 设置绘图格式
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1: # 当子图为1×1
            self.axes = [self.axes, ]
        # 使用lambda函数捕获参数
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)
```

下面实现一个训练函数，在 `train_iter` 访问到的训练数据集中训练模型 `net`。每个训练周期结束时，都会使用测试集评估。同时，使用 `Animator` 来可视化训练进度：
```python
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
    """训练模型（定义见第3章）"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
```

设置学习率，并显示迭代过程：
```python
#学习率设置
lr = 0.1

def updater(batch_size):
    return d2l.sgd([W, b], lr, batch_size)

#训练    
num_epochs = 10
train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
```
![[Pasted image 20250315165550.png]]

### 3.6.7 预测
训练完模型后，我们可以进行预测：
```python
def predict_ch3(net, test_iter, n=6):  #@save
    """预测标签（定义见第3章）"""
    for X, y in test_iter:
        break
    trues = d2l.get_fashion_mnist_labels(y)
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
    d2l.show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])

predict_ch3(net, test_iter)
```

## 3.7 softmax 回归的简洁实现
本节使用深度学习框架的高级 API 来实现 softmax 回归：
```python
import torch
from torch import nn
from d2l import torch as d2l

#加载数据
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

### 3.7.1 初始化模型参数
```python
#PyTorch不会隐式地调整输入的形状。因此，我们在线性层前定义了展平层（flatten），来调整网络输入的形状
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

#定义初始化权重函数
def init_weights(m):
    if type(m) == nn.Linear: #只对线性层初始化
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights) #应用函数
```
这里的 `.apply()` 不是 `python` 的内置函数，而是 `torch.nn.Module` 的方法。在这里，它的作用是对 `Module` 对象 (此处为 `net`) 的所有子模块进行初始化

### 3.7.2 重新审视 Softmax 实现
原始的 Softmax 输出存在以下问题：
* 上溢：输入值 $o_k$ 过大，$\exp(o_k)$ 超出数值表示范围，显示为 `inf`
* 下溢：输入值 $o_k$ 过小，$\exp(o_k)$ 被舍入为 0，对数运算时 $\log(0) = -\infty$
解决方案：
* 对所有的 $o_k$，减去最大值 $\max (o_k)$，这一操作不会改变 softmax 结果，但可以避免上溢
$$
\hat{y}_j = \frac{\exp(o_j - \max(o_k)) \exp(\max(o_k))}{\sum_k \exp(o_k - \max(o_k)) \exp(\max(o_k))} \\
= \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}.
$$
* 计算交叉熵时，将 softmax 和交叉熵结合在一起，避免计算 $\exp(o_j - \max(o_k))$，直接使用 $o_j - \max(o_k)$
$$
\begin{align*}
\log(\hat{y}_j) &= \log\left(\frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}\right) \\
&= \log\left(\exp(o_j - \max(o_k))\right) - \log\left(\sum_k \exp(o_k - \max(o_k))\right) \\
&= o_j - \max(o_k) - \log\left(\sum_k \exp(o_k - \max(o_k))\right)
\end{align*}
$$

现代交叉熵计算器可以保留传统的 sotmax 函数，以用于评估模型输出概率；同时损失函数计算使用原始得分，防止溢出问题。这是通过“LogSumExp 技巧”实现的：
```python
#损失函数
loss = nn.CrossEntropyLoss(reduction='none')
```
### 3.7.3 优化算法
这里使用学习率为 $0.1$ 的小批量随机梯度下降算法：
```python
#创建优化算法对象
trainer = torch.optim.SGD(net.parameters(), lr=0.1)
```
### 3.7.4 训练
现在可以直接对模型进行训练了：
```python
#训练模型
num_epochs = 10 #迭代次数
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```
![[Pasted image 20250316112442.png]] m