
## 2.1 数据操作

**张量 (tensor)** 就是 n 维数组：
* `Numpy` 中，为数据类型 `ndarray`
* `Pytorch` 中，为数据类型 `Tensor`

下面的所有操作基于 `torch` 包
### 2.1.1-2.1.4 入门与运算

一些基础操作：
```python
x = torch.arange(12)
x.shape #获取大小
x.numel() #获取元素总数
X = x.reshape(3, 4) #修改形状
```

特殊矩阵创建：
```python
torch.zeros((2, 3, 4))
torch.ones((2, 3, 4))
torch.randn(3, 4) #随机矩阵, 服从高斯分布
```

`torch` 中加减乘除与一元运算 (如 `exp`) 无差异

连接矩阵：
```python
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0) #按行变动连接, 结果是6×4的矩阵
torch.cat((X, Y), dim=1) #按列变动连接, 结果是3×8的矩阵
```

布尔矩阵：
```python
X == Y
#输出结果
# tensor([[False,  True, False,  True],
#        [False, False, False, False],
#        [False, False, False, False]])
```

矩阵**广播 (broadcasting)**，可以对不同形状的张量操作：
```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a + b #广播为3×2矩阵
#输出
# tensor([[0, 1],
#        [1, 2],
#        [2, 3]]) 
```

索引和切片：
```python
X[-1] #倒数第一行
X[1:3] #二到三行
```
>  索引时也是左闭右开区间！

### 2.1.5 节省内存
导致重新分配内存的操作：
```python
before = id(Y)
Y = Y + X
id(Y) == before #输出False
```

不会重新分配内存的操作：
```python
Z = torch.zeros_like(Y) #一个和$Y$同形状的矩阵
print('id(Z):', id(Z))
Z[:] = X + Y #不会重新分配内存
print('id(Z):', id(Z))

before = id(X)
X += Y #不会重新分配内存
id(X) == before
```
## 2.2 数据预处理

假设我们得到的数据包括离散型数值和类别数值，且它们都包括缺失值。用插值法处理缺失值：
```python
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean()) #插值法
print(inputs)

inputs = pd.get_dummies(inputs, dummy_na=True) #用哑变量处理类别缺失值
print(inputs)
```
这里分别用均值和哑变量处理了连续型和离散型的缺失值
>  哑变量相当于把 NaN 看成一类

上面得到的数据基于 `pandas`，需要转换为 `tensor` 类型：
```python
X = torch.tensor(inputs.to_numpy(dtype=float))
y = torch.tensor(outputs.to_numpy(dtype=float))
```
转换后，可被 `torch` 接受

## 2.3 线性代数

两个相同形状矩阵的逐元素乘积为**hadamard 积**：
$$
\mathbf{A} \odot \mathbf{B} = \begin{bmatrix}
a_{11}b_{11} & a_{12}b_{12} & \cdots & a_{1n}b_{1n} \\
a_{21}b_{21} & a_{22}b_{22} & \cdots & a_{2n}b_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1}b_{m1} & a_{m2}b_{m2} & \cdots & a_{mn}b_{mn}
\end{bmatrix}
$$
矩阵降维：
```python
A_sum_axis1 = A.sum(axis=1) #按行求和降维
print(A_sum_axis1.shape)
A.mean(axis=0) #按列求平均值降维
```
这里直接将原来的张量降低了一个维度 (从 2 降低到 1)。若要保持张量形状：
```python
sum_A = A.sum(axis=1, #按行求和
keepidms=True) #指定保持维度

#输出
#tensor([[ 6.],  
#[22.],  
#[38.],  
#[54.],  
#[70.]])
```
>  注意这里 `axis=0` 表示按列

求累计和：
```python
A.cumsum(axis=0) #从上到下按列累计和, 形状同A
```

**点积 (dot product)** 作用：
1. 表示加权求和，由数值向量和权重向量组成
2. 当权重非负和为一，表示**加权平均 (weighted average)**
3. 夹角余弦值

向量的**范数 (norm)** 可用于表示向量的“大小”，取决于场合。向量范数的三性质：
1. 非负性  $f(\textbf{X}) \geq 0$
2. 齐次性 $f(\alpha \textbf{X}) = |\alpha| \textbf{X}$
3. 三角不等式 $f(\textbf{X} +\textbf{Y}) \leq f(\textbf{X}) + f(\textbf{Y})$

向量范数举例：
1. $L_1$ 范数 $\|\mathbf{x}\|_1 = \sum_{i=1}^n |x_i|$
2. $L_2$ 范数 $\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}$，常用 $L_2$ 范数的平方，$2$ 可以省略不写
3. $L_p$ 范数 $\| \textbf{X} \|_p = \left( \sum^n_{i=1} |x_i|^p \right)^{1/p}$

从向量范数扩展到 $n × m$ 矩阵的范数**Frobenius 范数**：
$$
\|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} x_{ij}^2}
$$
即所有元素平方和的开方

## 2.4-2.5 微积分和自动微分

拟合模型的任务分为两个关键：
1. **优化 (optimization)** 拟合观测数据
2. **泛化 (generalization)** 有效性超出训练集

多元函数对所有变量偏导数的向量就是**梯度(gradient)** 向量：
$$
\nabla_{\mathbf{x}} f(\mathbf{x}) = \left[ \frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n} \right]^\top
$$
对 $\forall \textbf{A} \in \mathbb{R}^{m \times n}$，多元微分函数包含以下规则：
1. $\nabla_{\textbf{x}} \textbf{A} \textbf{x} = \textbf{A}^T$
2. $\nabla_{\textbf{x}} \textbf{x}^T \textbf{A} = \textbf{A}$
3. $\nabla_{\textbf{x}} \textbf{x}^T \textbf{A} \textbf{x} = (\textbf{A} + \textbf{A}^T) \textbf{x}$
4. $\nabla_{\textbf{x}} \| \textbf{x} \|^2 = \nabla_{\textbf{x}} \textbf{x}^T \textbf{x} = 2 \textbf{x}$
上面的 $\textbf{x}$ 表示一个向量；对于矩阵 $\textbf{X}$，也都有类似第 4 条 
$$
\nabla_{\textbf{X}} \| \textbf{X} \|_F= 2 \textbf{X}
$$

深度学习框架通过自动计算导数 (**自动微分 (automatic differentialtion)**)，根据设计好的模型，系统构建**计算图 (computational graph)**，跟踪哪些数据通过哪些操作组合起来产生输出，随后使系统**反向传播 (backpropagate)** 梯度，即跟踪整个计算图，填充关于每个参数的偏导数

一个关于标量函数的示例。注意 $\textbf{x}$ 的梯度形状与之相同：
```python
import torch

#初始化向量参数
x = torch.arange(4.0)
x.requires_grad_(True)  #启用梯度跟踪, 设置属性为True
print(x.grad)  # 默认值是None

#创建标量函数, 注意与下一段代码的非标量函数区分
y = 2 * torch.dot(x, x)
y.backward() #计算梯度
print(x.grad) #输出梯度, y = 2x^2, dy/dx = 4x
print(x.grad == 4 * x) #验证

# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
x.grad.zero_()
y = x.sum() #新的函数, 所有元素求和
y.backward() 
print(x.grad) #输出[1, 1, 1, 1]
```

一个关于非标量函数的例子：
```python
# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。
# 本例只想求偏导数的和，所以传递一个1的梯度是合适的
x.grad.zero_()
y = x * x
# 等价于y.backward(torch.ones(len(x)))
y.sum().backward()
print(x.grad)
```

>  解释关于 `x * x` 和 `torch.dot(x, x)` 的区别：前者使用广播机制，返回与 `x` 等大的张量；后者则是必须要求 `x` 为一维向量，返回点积是一个标量


通过分离变量，我们可以将中间变量当做常数，只考虑底层变量的梯度计算：
```python
x.grad.zero_()
y = x * x
u = y.detach() #将y分离出来为u
z = u * x

#计算z的梯度, 实际上关于y的梯度
z.sum().backward()
print(x.grad == u)

#分离y后, 计算y关于x的梯度
x.grad.zero_()
y.sum().backward()
print(x.grad == 2 * x)
```
被分离的中间变量之后的变量 (示例中为 `x`) 不会在 $z$ 计算梯度时被计算，这里计算 `z` 的梯度时只计算到 `u`，也即是被分离变量 `y`

自动微分可在 `Python` 控制流中计算变量梯度：
```python
#定义函数f, 则函数f的输出取决于输入a
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c
    
#计算梯度
a = torch.randn(size=(), requires_grad=True) #size=()表明一个常数值a
d = f(a)
d.backward()
print(a.grad == d/a) #这里对分段常数函数f总有f = k*a
```

寻求帮助，可使用 `help(torch.ones)` 类似的命令；查看函数使用 `dir(torch)`

