
本章介绍深度学习 API 的一些高级功能

## 5.1 层和块

现代神经网络通常研究比单个层大但是比整个模型小的组件，我们通常称为**块 (block)**。它可以描述单个层、多个层构成的模块或模型本身。在 `python` 中，块表示为**类(class)**
>  ResNet-152 架构中包含数百个层，这些层由**层组 (groups of layers)** 组成

使用 `nn.Sequential()` 构建的模型实际上就是一个类的实例化，它维护了一个由 `Module` 对象组成的有序列表，并逐个调用，将前一个块的输出作为下一个块的输入
![[Pasted image 20250328131957.png]]
### 5.1.1 自定义块

块的基本功能：
1. 将输入数据作为其前向传播函数的参数。  
2. 通过前向传播函数来生成输出。请注意, 输出的形状可能与输入的形状不同。例如, 我们上面模型中的第一个全连接的层接收一个 20 维的输入, 但是返回一个维度为 256 的输出。  
3. 计算其输出关于输入的梯度, 可通过其反向传播函数进行访问。通常这是自动发生的。  
4. 存储和访问前向传播计算所需的参数。  
5. 根据需要初始化模型参数。

下面的代码实现了一个 `MLP` 类：
```python
from torch.nn import functional as F

class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__() # 继承
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))
```
这个类接受一个输入 `X`，并输出对应的值 `Y`：
```python
net = MLP()
print(net(X))
```
> `__init__(self)` 没有显式地接受输入变量，推测为 `nn.Module` 类的输入变量

### 5.1.2 顺序块
要定义简单的 `Sequential` 类，需要实现两个关键函数：
1. 一种将块逐个追加到列表中的函数
2. 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”
```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module # 用字典形式存储

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values(): # 获取字典的值而不是键
            X = block(X) # 迭代每一层的输出, 作为下一层输入
        return X
```
这里的 `self._modules` 是 `nn.Module` 的内部成员变量 `OrderedDict`，类似一个字典，但可以保证顺序与添加顺序一致

### 5.1.3 在前向传播函数中执行代码

之前的神经网络中的参数都是可更新的，会随着模型的学习而变化，我们也可引入不随着训练而更新的参数**常数参数 (constant parameter)**：
```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。因此其在训练期间保持不变
        self.rand_weight = torch.rand((20, 20), requires_grad=False) # 不计算梯度, 即控制训练期间保持不变
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # 使用创建的常量参数以及relu和mm函数
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # 复用全连接层。这相当于两个全连接层共享参数
        X = self.linear(X)
        # 控制流
        while X.abs().sum() > 1: # 当L1范数大于1时
            X /= 2
        return X.sum()
        
# 演示效果
net = FixedHiddenMLP()
net(X)
```
以上的 MLP 演示了如何在块中引入常数参数，并且引入 `python` 的控制流

下面的例子演示了块的嵌套：
```python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```

## 5.2 参数管理

本章我们将实现：
* 访问参数。对参数进行调试、诊断和可视化
* 参数初始化
* 在不同模型之间，共享参数

### 5.2.1 参数访问
用 `nn.Sequential()` 定义的模型，可直接用索引访问模型任意层：
```python
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
X = torch.rand(size=(2, 4))
net(X)
print(net[2].state_dict())
"""
输出:
OrderedDict([('weight', tensor([[-0.0427, -0.2939, -0.1894,  0.0220, -0.1709, -0.1522, -0.0334, -0.2263]])), ('bias', tensor([0.0887]))])
"""
```
这里输出的是一个元素为元组的列表，包含两个元组：
* `weight` 权重参数。这里访问第二层的权重，是一个 `(8,)` 的 `ten
* `bias` 偏置参数

实际上，模型每一层的各种参数都是一个对象：
```python
print(type(net[1].bias)) # 输出为<class 'torch.nn.parameter.Parameter'
print(net[1].bias) # 参数bias是一个对象
print(net[1].bias.data()) # 输出一个Tensor
```
还可以访问参数的梯度：
```python
print(net[2].weight.grad == None) # 输出True
```
这里参数的梯度为 `None`，是因为还没有用反向传播计算梯度

处理复杂的块，如嵌套块时，需要递归整个树来获取每个子块的参数：
```python
# 递归获取子块参数
print(*[(name, param.shape) for name, param in net[0].named_parameters()])
print(*[(name, param.shape) for name, param in net.named_parameters()])
```
这里的 `named_parameters()` 是层的一个方法，用于获取模型中所有参数的名称和对应的参数对象，返回一个生成器，生成元组 `(name, param)`，`param` 表示参数的数值
> `*` 是一个解包操作符，用于将列表中的元素解包为多个参数传递给函数

获取到参数的名称和大小后，可以用名称访问其数值：
```python
print(net.state_dict()['2.bias'].data) # 访问参数字典
```

在下面的嵌套：
```python
def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                         nn.Linear(8, 4), nn.ReLU())

def block2():
    net = nn.Sequential()
    for i in range(4):
        # 在这里嵌套
        net.add_module(f'block {i}', block1())
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))
print(rgnet(X))
```
中，我们可以打印网络实例，观察它的工作方式：
```python
print(rgnet)
"""
输出
Sequential(
  (0): Sequential(
    (block 0): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 1): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 2): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 3): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
  )
  (1): Linear(in_features=4, out_features=1, bias=True)
)
"""
```
对于这种多层嵌套的网络，可以像嵌套列表一样索引参数：
```python
print(rgnet[0][1][0].bias.data)
```
这里索引第一个块的第二个子块的第一层的偏置项数值

### 5.2.2 参数初始化
深度学习框架内置了自动的参数初始化。默认情况下，`pytorch` 会根据一个范围均匀地初始化权重和偏置矩阵，`nn.init` 模块提供了多种预制的初始化方法

下面的初始化器是内置的，可以将权重参数初始化为 $\sigma=0.01$ 的高斯随机变量，偏置参数设置为 0：
```python
# 自定义标准初始化参数函数
def init_normal(m):
    if type(m) == nn.Linear: # 只对nn.Linear初始化
        nn.init.normal_(m.weight, mean=0, std=0.01) # 标准化
        nn.init.zeros_(m.bias) # 置0
net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]
```
此外，也可设置为给定的常数，如 1：
```python
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 1) # 参数设置为常数1
        nn.init.zeros_(m.bias)
net.apply(init_constant)
net[0].weight.data[0], net[0].bias.data[0]
```
通过对不同的层作用 `.apply()` 方法，我们可以对不同层应用不同的初始化方法：
```python
def init_xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)
# 对不同层应用初始化方法
net[0].apply(init_xavier)
net[2].apply(init_42)
print(net[0].weight.data[0])
print(net[2].weight.data)
```

有时候，我们希望自定义初始化方法，例如我们希望用这个式子初始化权重：
$$
w\sim
\begin{cases}
U(5,10) & \text{可能性 }\frac{1}{4} \\
0 & \text{可能性 }\frac{1}{2} \\
U(-10,-5) & \text{可能性 }\frac{1}{4} & 
\end{cases}
$$
我们可以用下面的函数实现：
```python
def my_init(m):
    if type(m) == nn.Linear:
        print("Init", *[(name, param.shape)
                        for name, param in m.named_parameters()][0])
        nn.init.uniform_(m.weight, -10, 10)
        m.weight.data *= m.weight.data.abs() >= 5

net.apply(my_init)
net[0].weight[:2]
```

不论如何，我们都可以直接操作参数：
```python
net[0].weight.data[:] += 1
net[0].weight.data[0, 0] = 42
print(net[0].weight.data[0])
```

### 5.2.3 参数绑定
有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数：
```python
# 我们需要给共享层一个名称，以便可以引用它的参数
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(), # 第2-3层
                    shared, nn.ReLU(), # 第4-5层
                    nn.Linear(8, 1))
net(X)
# 检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] = 100
# 确保它们实际上是同一个对象，而不只是有相同的值
print(net[2].weight.data[0] == net[4].weight.data[0])
```
这里第 2 和 4 层都是 `shared` (序号从 0 开始)，它们是同一个对象，而不是对象的复制；因此修改其中一个的参数，会导致另一个层的参数也被修改
> 必须注意，由于两个层参数共享，梯度也会累积，即传播到第 4 个层时梯度累积一次，传播到第 2 个层时梯度累积第二次

## 5.3 延后初始化
原书出版时，`torch` 尚未拥有一个稳定的延后初始化版本。有文献提到 `nn.LazyLinear` 类可实现延后初始化，即在第一次调用时才会根据输入的 `tensor` 推断出 `in_features`，但仍需指定 `out_features`

## 5.4 自定义层
处理不同的任务可能涉及到 `torch` 框架中不存在的层，需要我们自定义

### 5.4.1 不带参数的层
我们可以构造不带参数的层。下面的层从输入中减去其均值（类似标准化）：
```python
import torch
import torch.nn.functional as F
from torch import nn

class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, X):
        return X - X.mean()
        
layer = CenteredLayer()
layer(torch.FloatTensor([1, 2, 3, 4, 5])) 

# 额外的检查, 随机数据的均值处理后应为0
Y = net(torch.rand(4, 8))
print(Y.mean())
```

有了这个层之后，我们可以将这个层作为组件合并到更复杂的模型中：
```python
net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())
```

### 5.4.2 带参数的层
下面实现自定义的全连接层。该层需要两个参数，分别是权重和偏置；此外，接受输入特征维度和输出特征维度两个变量：
```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
```
这里使用了 `nn.Parameter` 来构造权重和偏置，使得它们成为可被 `torch` 框架识别的对象，能在迭代中优化

实例化 `MyLinear` 类：
```python
linear = MyLinear(5, 3)
print(linear.weight)
```
进行前向传播：
```python
print(linear(torch.rand(2, 5)))

# 将自定义层用在Sequential中
net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))
print(net(torch.rand(2, 64)))
```
## 5.5 读写文件

有时我们希望保存训练的模型，以备将来在各种环境中使用（比如在部署中进行预测）。此外，当运行一个耗时较长的训练过程时，最佳的做法是定期保存中间结果，以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果

### 5.5.1 加载和保存张量

对于单个 `tensor`，我们可用 `load()` 和 `save()` 函数分别读写：
```python
import torch
from torch import nn
from torch.nn import functional as F

x = torch.arange(4)
torch.save(x, 'x-file')

x2 = torch.load('x-file')
print(x2)
```
也可将张量组合为列表，然后读取回内存：
```python
y = torch.zeros(4)
torch.save([x, y],'x-files')
x2, y2 = torch.load('x-files')
print(x2, y2)
```
映射到字典再存取也可以：
```python
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict') # 给字典添加别名
mydict2 = torch.load('mydict') # 按字典别名读取
print(mydict2)
```

### 5.5.2 加载和保存模型参数

`torch` 提供内置参数保存整个神经网络的参数 (并不是整个模型代码)：
```python
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))

# 创建一个MLP
net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)

# 保存到mlp.params文件
torch.save(net.state_dict(), 'mlp.params')

# 获取全新的网络, 并加载参数
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval()

# 验证模型输出
Y_clone = clone(X)
print(Y_clone == Y)
```

## 5.6 GPU

查看显卡信息：
```powershell
nvidia-smi
```
输出：
```powershell
Fri Mar 28 19:51:17 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.97                 Driver Version: 555.97         CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0  On |                  N/A |
| N/A   57C    P4             12W /  102W |    1610MiB /   8188MiB |     27%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      7728    C+G   ...ft Office\root\Office16\WINWORD.EXE      N/A      |
|    0   N/A  N/A     14716    C+G   ...5n1h2txyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A     19068    C+G   ...GeForce Experience\NVIDIA Share.exe      N/A      |
|    0   N/A  N/A     35980    C+G   ...ekyb3d8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A     64264    C+G   C:\Program Files\Zotero\zotero.exe          N/A      |
|    0   N/A  N/A     70988    C+G   ...t.LockApp_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A     75416    C+G   ...les\Microsoft OneDrive\OneDrive.exe      N/A      |
|    0   N/A  N/A     82968    C+G   ....7122.0_x64__8wekyb3d8bbwe\Todo.exe      N/A      |
|    0   N/A  N/A     83172    C+G   C:\Windows\explorer.exe                     N/A      |
|    0   N/A  N/A     88864    C+G   C:\Program Files\Tencent\QQNT\QQ.exe        N/A      |
|    0   N/A  N/A     91332    C+G   ...m\radeonsoftware\RadeonSoftware.exe      N/A      |
|    0   N/A  N/A     91652    C+G   F:\CloudMusic\cloudmusic.exe                N/A      |
|    0   N/A  N/A     92688    C+G   ...2txyewy\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A     93104    C+G   ...s\System32\ApplicationFrameHost.exe      N/A      |
|    0   N/A  N/A     95084    C+G   ...nt.CBS_cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A     97736    C+G   F:\Obsidian\Obsidian.exe                    N/A      |
|    0   N/A  N/A     98216    C+G   ...CBS_cw5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A     98696    C+G   C:\Program Files\Zotero\zotero.exe          N/A      |
|    0   N/A  N/A    100904    C+G   ...nr4m\radeonsoftware\AMDRSSrcExt.exe      N/A      |
|    0   N/A  N/A    103612    C+G   ...on\134.0.3124.85\msedgewebview2.exe      N/A      |
|    0   N/A  N/A    103932    C+G   ...siveControlPanel\SystemSettings.exe      N/A      |
|    0   N/A  N/A    107640    C+G   ...1\extracted\runtime\WeChatAppEx.exe      N/A      |
|    0   N/A  N/A    114632    C+G   C:\Windows\System32\ShellHost.exe           N/A      |
+-----------------------------------------------------------------------------------------+
```
> 这是笔者家的 GPU，不是作者的

观察输出，每个数组是一个**设备 (device)**，通常称为**环境 (context)**。通过智能地将数组分配给环境，我们可以最大限度地减少在设备之间传输数据的时间

下面的程序至少需要 2 个 GPU 才能运行

### 5.6.1 计算设备
`torch` 中的设备可用 `torch.device('cpu')` 和 `torch.device('cuda')` 分别表示 CPU 和 GPU。其中 CPU 只有一个，包括所有 CPU 和内存，GPU 可有多个：
```python
import torch
from torch import nn

torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')
``` 
注意上面用 `torch.device(f'cuda:{i}')` 表示第 $i$ 块 GPU，`cuda:0` 和 `cuda` 表示等价

查询可用数量：
```python
print(torch.cuda.device_count())
```
用不同的 GPU 运行代码：
```python
def try_gpu(i=0):
    """如果存在，则返回gpu(i)，否则返回cpu()"""
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')

def try_all_gpus():  #@save
    """返回所有可用的GPU，如果没有GPU，则返回[cpu(),]"""
    devices = [torch.device(f'cuda:{i}')
             for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]

try_gpu(), try_gpu(10), try_all_gpus()
```
这两个函数分别允许我们定义环境中的设备使用

### 5.6.2 张量与 GPU
`tensor` 有一个 `device` 属性，可查询它位于哪个设备：
```python
x = torch.tensor([1, 2, 3])
print(x.device)
```
下面的代码可将 `tensor` 存储在 GPU 上：
```python
# 尝试GPU存储
X = torch.ones(2, 3, device=try_gpu())
print(X)

# 尝试存储在第1块, 序号从0开始的GPU上
Y = torch.rand(2, 3, device=try_gpu(1))
print(Y)
```

在进行运算时，需要额外注意 `tensor` 的位置。简单地执行运算可能会让设备找不到 `tensor` 而报错：
```python
Z = X.cuda(1)
print(X)
print(Z)
"""
输出
tensor([[1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
tensor([[1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
"""

# 同一设备方可相加
print(Y + Z)

# 同意设备上不会重新分配内存
print(Z.cuda(1) is Z) # 输出true
```

### 5.6.3 神经网络与 GPU
类似地，神经网络也可以指定设备存储：
```python
net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=try_gpu()) # 转移net到gpu上
``` 
调用这个网络时，将在同一个设备输出结果：
```python
net(X)

# 确认设备存储情况
print(net[0].weight.data.device)
```


